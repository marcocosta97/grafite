# High-level description of files

* `dst.h` and `dst.cpp`: Core code of Rosetta and general filter api (integrated with SuRF).
* `run_workload.cpp`: Standalone experiments
* `disk.h`: Helper for disk-resident standalone experiments. Included in `run_workload.cpp` only if relevant.
* `run_experiments.sh`: Script that runs standalone experiments
* `run_rocksdb_experiments.sh`: Script that runs rocksdb-dependent experiments

# Running experiments

## Standalone

Generally experiments can be run using `run_workload`, which is compiled
with `make run_workload`. However, `run_experiments.sh` can
batch/parallelize multiple experiments and make the process easier.
Nevertheless, `run_workload` takes the following parameters (named as in the
usage message):

* `keys`: Path to keys file (`data.txt`) as generated by the workload generator
* `queries-left`: Path to `txn.txt`
* `queries-right`: Path to `upper_bound.txt`
* `bits-per-key`: Bits per key to use (**only honored by Rosetta**)
* `SuRF or DST`: Name of filter to use
* `surf hash length`: If using SuRF, the length of SuRF-Hash
* `surf real length`: If using SuRF, the length of SuRF-Real
* `dfs-diffidence`: If using DST, the old diffidence (Deprecated, set to large value)
* `bfs-diffidence`: If using DST, diffidence
* `cutoff`: Number of levels to cut off from bottom. Set to 0 to disable.

The experiment parameters are tweaked in `run_experiments.sh` as bash-style
arrays. Currently the parameters expressed by these arrays are:

* `nkeys`: Number of keys inserted/generated
* `keylen`: Length of generated keys in bits
* `nqrys`: Number of queries
* `kdist`: Key distribution ("uniform", "normal", "zipfian")
* `qdist`: Query distribution ("uniform", "normal", "zipfian", "adversary", "correlated")
* `mrange`: Maximum queried range
* `dfsdiff`: Dfs (old) diffidence (Deprecated, always set to a large number)
* `bfsdiff`: Diffidence
* `pqratio`: Ratio of point queries generated (given as float)
* `membudg`: Memory budget. **Notice that SuRF does not respect this**
* `surfhlen`: SuRF-Hash length
* `surfrlen`: SuRF-Real length
* `corrd`: Correlation degree. Only used if `qdist="correlated"`, otherwise set to any value
* `cutoff`: Number of levels to cut off from bottom. To turn off set to zero.

Another relevant variable is `MAX_JOBS`, which is defined at the top of the
file. This variable sets the maximum number of experiments that will be run at
the same time.

To parse the output of `run_experiments.sh`, pipe it to
`diffident-paper/rocksdb_exper_scripts/parse_standalone.py`.

### Strings

To compile the string experiments (WIP), use `make run_workload_string`. This
compiles `run_workload.cpp` with `#define KEYTYPE string` (instead of the
default `#define KEYTYPE uint64_t`). To run the experiment, the workload must
be generated using SuRF's benchmark generator, supplied with an email list.

### Standalone Disk

To compile the standalone disk experiment (WIP), use `make run_workload_disk`.
This compiles `run_workload.cpp` with `#define USE_DISK`. In this case, instead
of measuring FPR using an in-memory STL set, keys are saved to files, which are
queried. This should work if if `run_workload` is replaced by
`run_workload_disk` in `run_experiments.sh`, but hasn't been extensively tested
yet.

The file `disk.h` defines the `DiskBlock` data structure, whose template
argument is the key type (usually either `uint64_t` or `string`). Its
constructor parameters are the inserted keys, the file name, and the key pad
size in bytes. In order to simulate adding values, use the key pad size of 1024
bytes. This key pad size should also be enough to accomodate variable-length
strings. The same file defines `MAX_BUFFER_SIZE`, which indicates how many
elements at most should be inserted to a file. For example, if we want the
files to be 64MB and our key padding size is 1024 bytes, we will use `#define
MAX_BUFFER_SIZE (64 * 1024 * 1024 / 1024)`.

### Standalone Disk with Strings

To compile the standalone disk experiment with strings, use `make
run_workload_string_disk`, which will enable the changes in the two previous
paragraphs. To run, use the script `run_string_disk_experiments.sh`.

## RocksDB

The RocksDB experiment source is in the `rocksdb-surf-dst` repo. The two repos
(`rocksdb-surf-dst` and `diffident-paper`) should be in the same folder,
otherwise the Makefile inside `rocksdb-surf-dst/examples/` needs to be
configured accordingly.

To compile the RocksDB experiments, use `make query` inside
`rocksdb-surf-dst/examples/`. This should also compile `dst.o` inside
`diffident-paper/standalone` if necessary, but you may want to re-compile it
manually if any problems arise. If you are compiling for the first time, you
need to do `make -j8 static_lib` inside `rocksdb-surf-dst/` before compiling
the experiments. If things are weird, consider doing `make clean` in
`rocksdb-surf-dst/` and then re-doing `make -j8 static_lib`.

To run the RocksDB experiments, use `run_rocksdb_experiments.sh`. As with the
standalone experiments, this helps run multiple experiments, as well as not
having to remember the parameters that `query` needs.
`run_rocksdb_experiments.sh` takes the following command-line arguments:

* `path to experiment binary`: Path to `rocksdb-surf-dst/examples/query`
* `path to workload generator binary`: Path to `diffident-paper/workload_gen/workload`
* `scratch directory to perform experiments in`: A directory in which the rocksdb databases as well as the results will be stored for each experiment. This can be used to control whether the database is on HDD, SSD, or in memory (using tempfs).
* `scratch directory to store experiment data in`: In order to keep the workload consistent across filters, workloads are cached. This directory will hold the cached workloads by parameters.

As in the standalone experiments, the experiment parameters are controlled through bash-style arrays. They define:

* `nkeys`: Number of keys inserted/generated
* `keylen`: Length of generated keys in bits
* `nqrys`: Number of queries
* `kdist`: Key distribution ("uniform", "normal", "zipfian")
* `qdist`: Query distribution ("uniform", "normal", "zipfian", "adversary", "correlated")
* `mrange`: Maximum queried range
* `dfsdiff`: Dfs (old) diffidence (Deprecated, always set to a large number)
* `bfsdiff`: Diffidence
* `pqratio`: Ratio of point queries generated (given as float)
* `membudg`: Memory budget. **Notice that SuRF DOES respect this in RocksDB experiments**
* `surfhlen`: SuRF-Hash length (Deprecated due to parameter search)
* `surfrlen`: SuRF-Real length (Deprecated due to parameter search)
* `corrd`: Correlation degree. Only used if `qdist="correlated"`, otherwise set to any value
* `prefix`: Size of prefix in bytes for use in prefix filters

Note that as in standalone experiments, parallelization can be controlled using
`MAX_JOBS`, but it is advised to set it to 1 to avoid noise.

To parse the output of `run_rocksdb_experiments.sh`, pipe it to:
`diffident-paper/rocksdb_exper_scripts/parse_stats.py`.
